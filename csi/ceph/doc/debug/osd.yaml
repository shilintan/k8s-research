# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2023-12-22T01:31:39Z"
  generateName: rook-ceph-osd-0-6679d8b557-
  labels:
    app: rook-ceph-osd
    app.kubernetes.io/component: cephclusters.ceph.rook.io
    app.kubernetes.io/created-by: rook-ceph-operator
    app.kubernetes.io/instance: "0"
    app.kubernetes.io/managed-by: rook-ceph-operator
    app.kubernetes.io/name: ceph-osd
    app.kubernetes.io/part-of: rook-ceph
    ceph-osd-id: "0"
    ceph_daemon_id: "0"
    ceph_daemon_type: osd
    device-class: hdd
    failure-domain: k8s-10-88-201-61
    osd: "0"
    osd-store: bluestore
    pod-template-hash: 6679d8b557
    portable: "false"
    rook.io/operator-namespace: rook-ceph
    rook_cluster: rook-ceph
    topology-location-host: k8s-10-88-201-61
    topology-location-root: default
  name: rook-ceph-osd-0-6679d8b557-cqkhz
  namespace: rook-ceph
  ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-0-6679d8b557
      uid: 4e8d9325-0cbb-4a5f-a2c2-b7345b73e3bc
  resourceVersion: "120644"
  uid: 862fe36a-2a4d-4d7e-9844-978b22f5b326
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: shilintan.io/service-type-storage-ceph
                operator: In
                values:
                  - v
  containers:
    - args:
        - --foreground
        - --id
        - "0"
        - --fsid
        - 887ea7e7-3961-4752-9418-39587aee8618
        - --setuser
        - ceph
        - --setgroup
        - ceph
        - --crush-location=root=default host=k8s-10-88-201-61
        - --default-log-to-stderr=true
        - --default-err-to-stderr=true
        - --default-mon-cluster-log-to-stderr=true
        - '--default-log-stderr-prefix=debug '
        - --default-log-to-file=false
        - --default-mon-cluster-log-to-file=false
        - --ms-learn-addr-from-peer=false
      command:
        - ceph-osd
      env:
        - name: ROOK_NODE_NAME
          value: k8s-10-88-201-61
        - name: ROOK_CLUSTER_ID
          value: 607e22d1-8dcb-45ae-a79c-f11e6d826753
        - name: ROOK_CLUSTER_NAME
          value: rook-ceph
        - name: ROOK_PRIVATE_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: ROOK_PUBLIC_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          value: rook-ceph
        - name: ROOK_MON_ENDPOINTS
          valueFrom:
            configMapKeyRef:
              key: data
              name: rook-ceph-mon-endpoints
        - name: ROOK_CONFIG_DIR
          value: /var/lib/rook
        - name: ROOK_CEPH_CONFIG_OVERRIDE
          value: /etc/rook/config/override.conf
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: ROOK_CRUSHMAP_ROOT
          value: default
        - name: ROOK_CRUSHMAP_HOSTNAME
          value: k8s-10-88-201-61
        - name: CEPH_VOLUME_DEBUG
          value: "1"
        - name: CEPH_VOLUME_SKIP_RESTORECON
          value: "1"
        - name: DM_DISABLE_UDEV
          value: "1"
        - name: ROOK_OSDS_PER_DEVICE
          value: "1"
        - name: CONTAINER_IMAGE
          value: registry.cn-shanghai.aliyuncs.com/shilintan-public/quay.io_ceph_ceph:v18.2.0
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              divisor: "0"
              resource: limits.memory
        - name: POD_MEMORY_REQUEST
          valueFrom:
            resourceFieldRef:
              divisor: "0"
              resource: requests.memory
        - name: POD_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              divisor: "1"
              resource: limits.cpu
        - name: POD_CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              divisor: "0"
              resource: requests.cpu
        - name: CEPH_USE_RANDOM_NONCE
          value: "true"
        - name: ROOK_OSD_RESTART_INTERVAL
          value: "0"
        - name: ROOK_OSD_UUID
          value: 70c9e8aa-717a-4263-b12e-0417d922ede9
        - name: ROOK_OSD_ID
          value: "0"
        - name: ROOK_CEPH_MON_HOST
          valueFrom:
            secretKeyRef:
              key: mon_host
              name: rook-ceph-config
        - name: CEPH_ARGS
          value: -m $(ROOK_CEPH_MON_HOST)
        - name: ROOK_BLOCK_PATH
          value: /dev/vdd
        - name: ROOK_CV_MODE
          value: raw
        - name: ROOK_OSD_DEVICE_CLASS
          value: hdd
        - name: ROOK_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: ROOK_MSGR2
          value: msgr2_true_encryption_false_compression_false
      envFrom:
        - configMapRef:
            name: rook-ceph-osd-env-override
            optional: true
      image: registry.cn-shanghai.aliyuncs.com/shilintan-public/quay.io_ceph_ceph:v18.2.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
            - env
            - -i
            - sh
            - -c
            - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
          [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
          check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
          /g'\n\texit $rc\nfi\n"
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: osd
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      startupProbe:
        exec:
          command:
            - env
            - -i
            - sh
            - -c
            - "\noutp=\"$(ceph --admin-daemon /run/ceph/ceph-osd.0.asok status 2>&1)\"\nrc=$?\nif
          [ $rc -ne 0 ] && [ ! -f /tmp/osd-sleep ]; then\n\techo \"ceph daemon health
          check failed with the following output:\"\n\techo \"$outp\" | sed -e 's/^/>
          /g'\n\texit $rc\nfi\n"
        failureThreshold: 720
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /var/lib/rook
          name: rook-data
        - mountPath: /etc/ceph
          name: rook-config-override
          readOnly: true
        - mountPath: /run/ceph
          name: ceph-daemons-sock-dir
        - mountPath: /var/log/ceph
          name: rook-ceph-log
        - mountPath: /var/lib/ceph/crash
          name: rook-ceph-crash
        - mountPath: /dev
          name: devices
        - mountPath: /run/udev
          name: run-udev
        - mountPath: /var/lib/ceph/osd/ceph-0
          name: activate-osd
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-mqpvs
          readOnly: true
      workingDir: /var/log/ceph
    - command:
        - /bin/bash
        - -x
        - -e
        - -m
        - -c
        - "\nCEPH_CLIENT_ID=ceph-osd.0\nPERIODICITY=daily\nLOG_ROTATE_CEPH_FILE=/etc/logrotate.d/ceph\nLOG_MAX_SIZE=500M\nROTATE=7\n\n#
      edit the logrotate file to only rotate a specific daemon log\n# otherwise we
      will logrotate log files without reloading certain daemons\n# this might happen
      when multiple daemons run on the same machine\nsed -i \"s|*.log|$CEPH_CLIENT_ID.log|\"
      \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace default daily with given user input\nsed
      --in-place \"s/daily/$PERIODICITY/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\n# replace
      rotate count, default 7 for all ceph daemons other than rbd-mirror\nsed --in-place
      \"s/rotate 7/rotate $ROTATE/g\" \"$LOG_ROTATE_CEPH_FILE\"\n\nif [ \"$LOG_MAX_SIZE\"
      != \"0\" ]; then\n\t# adding maxsize $LOG_MAX_SIZE at the 4th line of the logrotate
      config file with 4 spaces to maintain indentation\n\tsed --in-place \"4i \\
      \\ \\ \\ maxsize $LOG_MAX_SIZE\" \"$LOG_ROTATE_CEPH_FILE\"\nfi\n\nwhile true;
      do\n\t# we don't force the logrorate but we let the logrotate binary handle
      the rotation based on user's input for periodicity and size\n\tlogrotate --verbose
      \"$LOG_ROTATE_CEPH_FILE\"\n\tsleep 15m\ndone\n"
      image: registry.cn-shanghai.aliyuncs.com/shilintan-public/quay.io_ceph_ceph:v18.2.0
      imagePullPolicy: IfNotPresent
      name: log-collector
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      tty: true
      volumeMounts:
        - mountPath: /etc/ceph
          name: rook-config-override
          readOnly: true
        - mountPath: /run/ceph
          name: ceph-daemons-sock-dir
        - mountPath: /var/log/ceph
          name: rook-ceph-log
        - mountPath: /var/lib/ceph/crash
          name: rook-ceph-crash
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-mqpvs
          readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  initContainers:
    - command:
        - /bin/bash
        - -c
        - "\nset -o errexit\nset -o pipefail\nset -o nounset # fail if variables are unset\nset
      -o xtrace\n\nOSD_ID=\"$ROOK_OSD_ID\"\nCEPH_FSID=887ea7e7-3961-4752-9418-39587aee8618\nOSD_UUID=70c9e8aa-717a-4263-b12e-0417d922ede9\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\n\n#
      \"ceph.conf\" must have the \"fsid\" global configuration to activate encrypted
      OSDs\n# after the following Ceph's PR is merged.\n# https://github.com/ceph/ceph/commit/25655e5a8829e001adf467511a6bde8142b0a575\n#
      This limitation will be removed later. After that, we can remove this\n# fsid
      injection code. Probably a good time is when to remove Quincy support.\n# https://github.com/rook/rook/pull/10333#discussion_r892817877\ncp
      --no-preserve=mode /etc/temp-ceph/ceph.conf /etc/ceph/ceph.conf\npython3 -c
      \"\nimport configparser\n\nconfig = configparser.ConfigParser()\nconfig.read('/etc/ceph/ceph.conf')\n\nif
      not config.has_section('global'):\n    config['global'] = {}\n\nif not config.has_option('global','fsid'):\n
      \   config['global']['fsid'] = '$CEPH_FSID'\n\nwith open('/etc/ceph/ceph.conf',
      'w') as configfile:\n    config.write(configfile)\n\"\n\n# create new keyring\nceph
      -n client.admin auth get-or-create osd.\"$OSD_ID\" mon 'allow profile osd' mgr
      'allow profile osd' osd 'allow *' -k /etc/ceph/admin-keyring-store/keyring\n\n#
      active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
      -d)\n\n\t# activate osd\n\tceph-volume lvm activate --no-systemd \"$OSD_STORE_FLAG\"
      \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory to a temporary directory\n\t#
      this is needed because when the init container exits, the tmpfs goes away and
      its content with it\n\t# this will result in the emptydir to be empty when accessed
      by the main osd container\n\tcp --verbose --no-dereference \"$OSD_DATA_DIR\"/*
      \"$TMP_DIR\"/\n\n\t# unmount the tmpfs since we don't need it anymore\n\tumount
      \"$OSD_DATA_DIR\"\n\n\t# copy back the content of the tmpfs into the original
      osd directory\n\tcp --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t#
      retain ownership of files to the ceph user/group\n\tchown --verbose --recursive
      ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm --recursive
      --force \"$TMP_DIR\"\nelse\n\t# 'ceph-volume raw list' (which the osd-prepare
      job uses to report OSDs on nodes)\n\t#  returns user-friendly device names which
      can change when systems reboot. To\n\t# keep OSD pods from crashing repeatedly
      after a reboot, we need to check if the\n\t# block device we have is still correct,
      and if it isn't correct, we need to\n\t# scan all the disks to find the right
      one.\n\tOSD_LIST=\"$(mktemp)\"\n\n\tfunction find_device() {\n\t\t# jq would
      be preferable, but might be removed for hardened Ceph images\n\t\t# python3
      should exist in all containers having Ceph\n\t\tpython3 -c \"\nimport sys, json\nfor
      _, info in json.load(sys.stdin).items():\n\tif info['osd_id'] == $OSD_ID:\n\t\tprint(info['device'],
      end='')\n\t\tprint('found device: ' + info['device'], file=sys.stderr) # log
      the disk we found to stderr\n\t\tsys.exit(0)  # don't keep processing once the
      disk is found\nsys.exit('no disk found with OSD ID $OSD_ID')\n\"\n\t}\n\n\tceph-volume
      raw list \"$DEVICE\" > \"$OSD_LIST\"\n\tcat \"$OSD_LIST\"\n\n\tif ! find_device
      < \"$OSD_LIST\"; then\n\t\tceph-volume raw list > \"$OSD_LIST\"\n\t\tcat \"$OSD_LIST\"\n\n\t\tDEVICE=\"$(find_device
      < \"$OSD_LIST\")\"\n\tfi\n\t[[ -z \"$DEVICE\" ]] && { echo \"no device\" ; exit
      1 ; }\n\n\t# If a kernel device name change happens and a block device file\n\t#
      in the OSD directory becomes missing, this OSD fails to start\n\t# continuously.
      This problem can be resolved by confirming\n\t# the validity of the device file
      and recreating it if necessary.\n\tOSD_BLOCK_PATH=/var/lib/ceph/osd/ceph-$OSD_ID/block\n\tif
      [ -L $OSD_BLOCK_PATH -a \"$(readlink $OSD_BLOCK_PATH)\" != $DEVICE ] ; then\n\t\trm
      $OSD_BLOCK_PATH\n\tfi\n\n\t# ceph-volume raw mode only supports bluestore so
      we don't need to pass a store flag\n\tceph-volume raw activate --device \"$DEVICE\"
      --no-systemd --no-tmpfs\nfi\n"
      env:
        - name: CEPH_VOLUME_DEBUG
          value: "1"
        - name: CEPH_VOLUME_SKIP_RESTORECON
          value: "1"
        - name: DM_DISABLE_UDEV
          value: "1"
        - name: ROOK_CEPH_MON_HOST
          valueFrom:
            secretKeyRef:
              key: mon_host
              name: rook-ceph-config
        - name: CEPH_ARGS
          value: -m $(ROOK_CEPH_MON_HOST)
        - name: ROOK_BLOCK_PATH
          value: /dev/vdd
        - name: ROOK_METADATA_DEVICE
        - name: ROOK_WAL_DEVICE
        - name: ROOK_OSD_ID
          value: "0"
      envFrom:
        - configMapRef:
            name: rook-ceph-osd-env-override
            optional: true
      image: registry.cn-shanghai.aliyuncs.com/shilintan-public/quay.io_ceph_ceph:v18.2.0
      imagePullPolicy: IfNotPresent
      name: activate
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /var/lib/ceph/osd/ceph-0
          name: activate-osd
        - mountPath: /dev
          name: devices
        - mountPath: /etc/temp-ceph
          name: rook-config-override
          readOnly: true
        - mountPath: /etc/ceph/admin-keyring-store/
          name: rook-ceph-admin-keyring
          readOnly: true
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-mqpvs
          readOnly: true
    - args:
        - bluefs-bdev-expand
        - --path
        - /var/lib/ceph/osd/ceph-0
      command:
        - ceph-bluestore-tool
      image: registry.cn-shanghai.aliyuncs.com/shilintan-public/quay.io_ceph_ceph:v18.2.0
      imagePullPolicy: IfNotPresent
      name: expand-bluefs
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /var/lib/ceph/osd/ceph-0
          name: activate-osd
        - mountPath: /dev
          name: devices
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-mqpvs
          readOnly: true
    - args:
        - --verbose
        - --recursive
        - ceph:ceph
        - /var/log/ceph
        - /var/lib/ceph/crash
        - /run/ceph
      command:
        - chown
      image: registry.cn-shanghai.aliyuncs.com/shilintan-public/quay.io_ceph_ceph:v18.2.0
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /var/lib/rook
          name: rook-data
        - mountPath: /etc/ceph
          name: rook-config-override
          readOnly: true
        - mountPath: /run/ceph
          name: ceph-daemons-sock-dir
        - mountPath: /var/log/ceph
          name: rook-ceph-log
        - mountPath: /var/lib/ceph/crash
          name: rook-ceph-crash
        - mountPath: /dev
          name: devices
        - mountPath: /run/udev
          name: run-udev
        - mountPath: /var/lib/ceph/osd/ceph-0
          name: activate-osd
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-mqpvs
          readOnly: true
  nodeName: k8s-10-88-201-61
  nodeSelector:
    kubernetes.io/hostname: k8s-10-88-201-61
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: rook-ceph-osd
  serviceAccountName: rook-ceph-osd
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
          - configMap:
              items:
                - key: config
                  mode: 292
                  path: ceph.conf
              name: rook-config-override
    - hostPath:
        path: /var/lib/rook/exporter
        type: DirectoryOrCreate
      name: ceph-daemons-sock-dir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/887ea7e7-3961-4752-9418-39587aee8618_70c9e8aa-717a-4263-b12e-0417d922ede9
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-admin-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-admin-keyring
    - name: kube-api-access-mqpvs
      projected:
        defaultMode: 420
        sources:
          - serviceAccountToken:
              expirationSeconds: 3607
              path: token
          - configMap:
              items:
                - key: ca.crt
                  path: ca.crt
              name: kube-root-ca.crt
          - downwardAPI:
              items:
                - fieldRef:
                    apiVersion: v1
                    fieldPath: metadata.namespace
                  path: namespace
